{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will introduce you to the basics of analysing words. \n",
    "You'll learn how to preprocess and represent words.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend of symbols:\n",
    "\n",
    "- ü§ì: Tips\n",
    "\n",
    "- ü§ñüìù: Your turn\n",
    "\n",
    "- ‚ùì: Question\n",
    "\n",
    "- üí´: Extra exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Word vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll learn how to transform words into vectors. Let's start with one-hot encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library **<tt> sklearn <tt>** has a function that transforms categorical features to one-hot vectors:\n",
    "    \n",
    "üåç https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html \n",
    "\n",
    "üåç https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the functions we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can' 'i' 'eat' 'the' 'pizza' 'you' 'can' 'eat' 'the' 'pizza']\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"Can I eat the Pizza\".lower()\n",
    "sent2 = \"You can eat the Pizza\".lower()\n",
    "\n",
    "doc1 = sent1.split()\n",
    "doc2 = sent2.split()\n",
    "\n",
    "doc1_array = array(doc1)\n",
    "doc2_array = array(doc2)\n",
    "\n",
    "doc3 = doc1+doc2\n",
    "data = list(doc3)\n",
    "\n",
    "values = array(data)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What does this code do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will transform words to numbers based on its position. To do so, we will use the **<tt> LabelEncoder() <tt>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 4 3 5 0 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['can', 'eat', 'i', 'pizza', 'the', 'you']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shows the order of words in the matrix\n",
    "list(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù **Your turn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the news dataset and calculate the onehot encoding of the first new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = array(df['corpus'][0].lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Word embeddings (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a word2vec model in Python is scarily easy with **<tt> gensim <tt>**.\n",
    "    \n",
    "üåç https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will train the word2vec model taking a look to the parameters:\n",
    "\n",
    "üåç https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will prepare the data. To do so, we need to transform every sentence in a list within a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"Can I eat the Pizza\".lower()\n",
    "sent2 = \"You can eat the Pizza\".lower()\n",
    "\n",
    "doc1 = sent1.split()\n",
    "doc2 = sent2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = [doc1, doc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(doc3, size=300, window=3, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=6, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can analyse the vocabulary of this word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'can': <gensim.models.keyedvectors.Vocab object at 0x7f8f48567340>, 'i': <gensim.models.keyedvectors.Vocab object at 0x7f8f48567370>, 'eat': <gensim.models.keyedvectors.Vocab object at 0x7f8f485673d0>, 'the': <gensim.models.keyedvectors.Vocab object at 0x7f8f48567430>, 'pizza': <gensim.models.keyedvectors.Vocab object at 0x7f8f48567490>, 'you': <gensim.models.keyedvectors.Vocab object at 0x7f8f485674f0>}\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyse the embeddings by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-49407fed89b0>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model['pizza']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.21682659e-04, -3.14061675e-04, -1.22502475e-04, -6.77091593e-04,\n",
       "        4.40915865e-05, -1.07827841e-03, -2.13238396e-04, -1.36438129e-03,\n",
       "        6.36198558e-04,  1.65240408e-03,  1.22291676e-03, -1.22182979e-03,\n",
       "        1.65365881e-03,  1.31142489e-03,  1.43662468e-03, -2.89088086e-04,\n",
       "       -7.97189830e-04, -2.57448759e-04,  6.80498779e-04,  8.74991994e-04,\n",
       "        8.59063934e-04,  1.98715148e-04,  3.23505665e-04, -4.19695774e-04,\n",
       "        7.28339481e-04,  9.47904889e-04, -1.20824773e-03,  4.65748017e-04,\n",
       "        9.54837073e-04,  3.14157922e-04, -9.68350214e-04, -1.46815274e-03,\n",
       "       -8.42247391e-04, -1.57231512e-03,  5.23636001e-04, -5.77613304e-04,\n",
       "        1.55919068e-03, -3.12595475e-05, -6.59398735e-04,  1.42673869e-03,\n",
       "        4.94487875e-04, -1.11329160e-03, -1.07714883e-03,  1.42718409e-03,\n",
       "       -1.36631844e-03,  8.11703270e-04,  8.10952857e-04, -4.45070764e-04,\n",
       "       -1.12503232e-03,  1.06437982e-03,  1.14634447e-03, -1.77222391e-04,\n",
       "       -1.32265943e-03,  1.30772660e-03, -4.48062987e-04,  1.33606920e-03,\n",
       "        2.16514149e-04, -5.39162662e-04,  1.28858571e-03, -6.46231929e-04,\n",
       "        7.78561574e-04, -1.57982088e-03,  8.69141513e-05, -1.50810159e-03,\n",
       "       -5.51462057e-04, -9.70874098e-04,  1.36566756e-03, -6.58462232e-04,\n",
       "       -6.58918405e-04, -7.88957463e-04,  7.42873526e-05,  1.24083995e-03,\n",
       "       -4.39219817e-04,  1.68290979e-04, -1.47841778e-03, -2.29272282e-05,\n",
       "       -7.04619801e-04, -7.46721125e-05,  9.39351215e-04, -7.28270272e-04,\n",
       "       -8.93376826e-04, -1.64373114e-03,  8.01430084e-04, -1.16304647e-04,\n",
       "       -2.45405710e-04,  7.16277922e-04,  8.63676076e-04,  1.29725737e-03,\n",
       "        1.43291359e-03, -6.77163713e-04, -9.57229524e-04,  7.15665461e-04,\n",
       "       -9.25451823e-05, -9.93010937e-04, -6.12533244e-04, -2.24322343e-04,\n",
       "       -1.00018771e-03,  6.18691556e-04, -1.30328757e-03, -9.86918807e-04,\n",
       "       -9.14619421e-04, -7.58161303e-04,  1.02107762e-03,  1.58467272e-03,\n",
       "        1.20012066e-03, -4.72039945e-04,  1.49995217e-03,  1.40308612e-03,\n",
       "        1.32610963e-03,  8.30696430e-04,  1.05617847e-03,  1.60360767e-03,\n",
       "       -3.12081684e-04,  1.32414571e-03, -7.55093643e-04, -8.88667535e-04,\n",
       "        3.78005672e-04, -1.22214470e-03, -1.49807963e-03,  4.83598298e-04,\n",
       "       -1.52593618e-03, -7.12659734e-04, -1.05446379e-03, -1.35142007e-03,\n",
       "        1.11207843e-03,  2.03034360e-04, -1.04728923e-03,  8.80061765e-04,\n",
       "       -1.61525223e-03, -6.14403398e-04,  3.14029458e-04, -5.48665295e-04,\n",
       "       -3.56309203e-04,  4.08164124e-05, -7.93906627e-04,  4.74161032e-04,\n",
       "        9.02914791e-04, -1.31776836e-03, -8.47803545e-04, -1.08879083e-03,\n",
       "        5.24450734e-04,  8.05912365e-04,  2.61148787e-04, -2.18257250e-04,\n",
       "        3.05352412e-04,  1.04357593e-03,  1.22074224e-03, -2.85598235e-05,\n",
       "       -1.10383982e-04,  1.39220851e-03,  1.51136529e-03, -5.95023390e-04,\n",
       "        1.28024933e-03, -5.96061989e-04,  7.56022986e-04, -1.30460830e-03,\n",
       "       -6.72289229e-04,  8.26844480e-04,  6.63050916e-04, -1.17589382e-03,\n",
       "       -1.00367249e-03,  1.42509828e-03,  2.82180321e-04, -5.44406124e-04,\n",
       "       -6.73911360e-04, -1.23256736e-03, -1.77810187e-04,  1.06803294e-04,\n",
       "       -1.60168030e-03,  1.40993274e-04,  3.73770716e-04,  1.19011663e-03,\n",
       "        9.86076891e-04,  1.65972210e-06,  6.96099654e-04, -2.62377231e-04,\n",
       "       -7.78924034e-04, -5.36314765e-05, -1.58219121e-03, -1.03667297e-03,\n",
       "       -1.53684767e-03,  1.48172351e-03,  6.84401311e-05,  1.07018510e-03,\n",
       "        1.05732644e-03,  1.24359410e-03, -2.14691536e-04, -1.09616551e-03,\n",
       "        3.82471451e-04, -2.12114639e-04,  1.04596757e-03,  2.89939751e-04,\n",
       "        8.66408984e-04, -1.19521737e-03,  6.40019192e-04, -1.11026620e-03,\n",
       "       -1.36168394e-03, -4.01717116e-04, -1.58986286e-03, -1.17870467e-03,\n",
       "        1.66177796e-03,  1.31429802e-03, -2.12687024e-04,  1.06463442e-03,\n",
       "        2.93698919e-04, -3.85331543e-04,  8.87649134e-04, -1.26410031e-03,\n",
       "       -1.51519768e-03, -7.98787980e-04, -9.94426431e-04,  1.15586491e-03,\n",
       "        4.46522288e-04,  8.26315081e-04, -1.32612581e-03, -1.15259325e-04,\n",
       "       -1.20101834e-03, -1.41802419e-04,  4.79748152e-04,  2.39172732e-04,\n",
       "        3.66798602e-04, -1.43630616e-03, -1.55690440e-03,  9.90835484e-04,\n",
       "       -3.54075310e-04, -1.57182233e-03, -1.19803112e-03, -7.22790894e-04,\n",
       "       -1.50481093e-04, -1.37135386e-03,  1.36200094e-03, -7.72982836e-04,\n",
       "       -1.62258279e-03,  4.31414432e-04,  3.99360608e-04, -1.29215140e-03,\n",
       "       -7.32980843e-04,  1.24718773e-03, -1.63454411e-03, -2.65773473e-04,\n",
       "       -4.99045011e-04, -1.05470547e-03, -3.59790050e-04, -2.62519843e-05,\n",
       "        4.58437455e-04, -8.00027512e-04, -3.05507594e-04, -1.22568151e-03,\n",
       "       -9.41600709e-04,  1.26504700e-03,  1.01706968e-03,  6.41983002e-04,\n",
       "        1.41816936e-03,  1.02328591e-03,  7.71479856e-04,  8.18936620e-04,\n",
       "       -1.21676363e-04, -7.60877883e-05, -1.48265180e-03, -9.12210671e-04,\n",
       "        1.55884621e-03, -1.58545887e-03, -1.38273998e-03,  4.33223759e-04,\n",
       "       -4.71625302e-04,  1.45056704e-03, -1.07120280e-03, -4.52841137e-04,\n",
       "        7.73679174e-04, -9.14037984e-04,  3.96212359e-04, -8.50416429e-04,\n",
       "        7.23990030e-04, -2.94966914e-04, -6.63652434e-04, -1.55190367e-03,\n",
       "        6.55290845e-04,  2.76155683e-04, -2.41179398e-04, -4.25183098e-05,\n",
       "        7.68321042e-04,  9.28958005e-04, -9.48075030e-04, -1.51471421e-03,\n",
       "       -1.64503523e-03, -1.54050195e-03,  1.28614344e-03,  1.57255062e-03,\n",
       "        1.91442567e-04,  9.50143323e-04, -1.40909935e-04, -1.45959097e-03,\n",
       "       -1.40703563e-03, -9.50039073e-04,  5.15176042e-04, -1.37263909e-03,\n",
       "        1.90746126e-04,  1.21789216e-03, -1.22397998e-03,  6.30477385e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['pizza']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using word2vec, we can analyse similarities across words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-a51c676a55fd>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar(positive=['pizza',], topn=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 0.07776791602373123)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['pizza',], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-244551ff3298>:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar(negative=['pizza',], topn=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('eat', 0.09031939506530762)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(negative=['pizza',], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And relations between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09031941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-0ae591619121>:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  print(model.similarity('pizza', 'eat'))\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('pizza', 'eat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ì Note that this model doesn't contain a lot of text, so it doesn't make sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù **Your turn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a word2vec embedding with the news corpus and extract the top 10 most similar words of *ultraviolet*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help to prepare the input for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'reindeer', 'is', 'the', 'emblematic', 'christmas', 'animal', 'and,', 'while', 'not', 'exactly', 'magical,', 'it', 'is', 'among', 'the', 'best', 'adapted', 'to', 'snowy', 'conditions.for', 'a', 'start,', 'a', 'reindeer‚Äôs', 'feet', 'have', 'four', 'toes', 'with', 'dewclaws', 'that', 'spread', 'out', 'to', 'distribute', 'its', 'weight', 'like', 'snowshoes,', 'and', 'are', 'equipped', 'with', 'sharp', 'hooves', 'for', 'digging', 'in', 'snow.a', 'reindeer‚Äôs', 'nose', 'warms', 'the', 'air', 'on', 'its', 'way', 'to', 'the', 'lungs,', 'cooling', 'it', 'again', 'before', 'it', 'is', 'exhaled.', 'as', 'well', 'as', 'retaining', 'heat,', 'this', 'helps', 'prevent', 'water', 'from', 'being', 'lost', 'as', 'vapour.', 'this', 'is', 'why', 'reindeer', 'breath', 'does', 'not', 'steam', 'like', 'human', 'and', 'horse', 'breath.a', 'reindeer‚Äôs', 'thick', 'double-layered', 'coat', 'is', 'so', 'efficient', 'that', 'it', 'is', 'more', 'likely', 'to', 'overheat', 'than', 'get', 'too', 'cold,', 'especially', 'when', 'running.', 'when', 'this', 'happens,', 'reindeer', 'pant', 'like', 'dogs', 'to', 'cool', 'down,', 'bypassing', 'the', 'nasal', 'heat', 'exchanger.snowfields', 'may', 'be', 'featureless', 'to', 'human', 'eyes,', 'but', 'reindeer', 'are', 'sensitive', 'to', 'ultraviolet', 'light,', 'an', 'evolutionary', 'development', 'that', 'only', 'occurred', 'after', 'the', 'animals', 'moved', 'to', 'arctic', 'regions.', 'snow', 'reflects', 'ultraviolet,', 'so', 'this', 'ultravision', 'allows', 'reindeer', 'to', 'spot', 'anything', 'lying', 'on', 'it,', 'in', 'particular', 'lichen,', 'which', 'they', 'eat,', 'and', 'traces', 'of', 'urine', 'showing', 'where', 'other', 'reindeer', 'have', 'passed.but', 'while', 'reindeer', 'thrive', 'in', 'christmas-card', 'weather,', 'they', 'are', 'increasingly', 'challenged', 'by', 'climate', 'change', 'and', 'the', 'freeze-thaw', 'conditions', 'that', 'produce', 'poor', 'grazing.']\n"
     ]
    }
   ],
   "source": [
    "# This is a loop that iterates over the dataframe\n",
    "news_vec = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    sent = row['corpus'].lower()\n",
    "    sent = sent.split()\n",
    "    news_vec.append(sent)    \n",
    " \n",
    "# Print the first element of the list:\n",
    "print(news_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí´ Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract the most similiar word to *climate*.\n",
    "- Calculate the similarity between *climate* and *weather*.\n",
    "- Calculate the most similar word to *huamanitarian* + *climate* - *droguth*.\n",
    "\n",
    "Does make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Word preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replicate the examples we have seen previously in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of separate symbols by introducing extra white space is called **tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = \"I've been 2 times to New York in 2011, but did not have the constitution for it. It DIDN'T appeal to me. I preferred Los Angeles.\"\n",
    "tokens = [[token.text for token in sentence] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What does this code do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', \"'ve\", 'been', '2', 'times', 'to', 'New', 'York', 'in', '2011', ',', 'but', 'did', 'not', 'have', 'the', 'constitution', 'for', 'it', '.'], ['It', \"DIDN'T\", 'appeal', 'to', 'me', '.'], ['I', 'preferred', 'Los', 'Angeles', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of reducing words to its dictionary based (lemma) is called **lemmatization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [[token.lemma_ for token in sentence] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-PRON-', 'have', 'be', '2', 'time', 'to', 'New', 'York', 'in', '2011', ',', 'but', 'do', 'not', 'have', 'the', 'constitution', 'for', '-PRON-', '.'], ['-PRON-', \"DIDN'T\", 'appeal', 'to', '-PRON-', '.'], ['-PRON-', 'prefer', 'Los', 'Angeles', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of reducing words to its stem is called **stemming**. \n",
    "\n",
    "This process is more radical than lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "stems = [[stemmer.stem(token) for token in sentence] for sentence in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 've', 'been', '2', 'time', 'to', 'new', 'york', 'in', '2011', ',', 'but', 'did', 'not', 'have', 'the', 'constitut', 'for', 'it', '.'], ['it', \"didn't\", 'appeal', 'to', 'me', '.'], ['i', 'prefer', 'los', 'angel', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4. Part of speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part of speech** corresponds to the process of classifying words to its category: nouns, verbs, adjectives, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [[token.pos_ for token in sentence] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['PRON', 'AUX', 'AUX', 'NUM', 'NOUN', 'ADP', 'PROPN', 'PROPN', 'ADP', 'NUM', 'PUNCT', 'CCONJ', 'AUX', 'PART', 'AUX', 'DET', 'NOUN', 'ADP', 'PRON', 'PUNCT'], ['PRON', 'PROPN', 'NOUN', 'ADP', 'PRON', 'PUNCT'], ['PRON', 'VERB', 'PROPN', 'PROPN', 'PUNCT']]\n"
     ]
    }
   ],
   "source": [
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5. Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords** is the process of removing words that cannot be beneficial for the analysis, like determiners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [[token.text for token in sentence if token.pos_ in {'NOUN', 'VERB', 'PROPN', 'ADJ', 'ADV'} and not token.is_stop]\n",
    "for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['times', 'New', 'York', 'constitution'], [\"DIDN'T\", 'appeal'], ['preferred', 'Los', 'Angeles']]\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative using **<tt> nltk <tt>** is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.6. Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing** is the process of classifying words in a sentence based on its syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('I', 'been', 'nsubj'),\n",
       "  (\"'ve\", 'been', 'aux'),\n",
       "  ('been', 'been', 'ROOT'),\n",
       "  ('2', 'times', 'nummod'),\n",
       "  ('times', 'been', 'npadvmod'),\n",
       "  ('to', 'been', 'prep'),\n",
       "  ('New', 'York', 'compound'),\n",
       "  ('York', 'to', 'pobj'),\n",
       "  ('in', 'been', 'prep'),\n",
       "  ('2011', 'in', 'pobj'),\n",
       "  (',', 'been', 'punct'),\n",
       "  ('but', 'been', 'cc'),\n",
       "  ('did', 'have', 'aux'),\n",
       "  ('not', 'have', 'neg'),\n",
       "  ('have', 'been', 'conj'),\n",
       "  ('the', 'constitution', 'det'),\n",
       "  ('constitution', 'have', 'dobj'),\n",
       "  ('for', 'constitution', 'prep'),\n",
       "  ('it', 'for', 'pobj'),\n",
       "  ('.', 'been', 'punct')],\n",
       " [('It', \"DIDN'T\", 'nsubj'),\n",
       "  (\"DIDN'T\", \"DIDN'T\", 'ROOT'),\n",
       "  ('appeal', \"DIDN'T\", 'ccomp'),\n",
       "  ('to', 'appeal', 'prep'),\n",
       "  ('me', 'to', 'pobj'),\n",
       "  ('.', \"DIDN'T\", 'punct')],\n",
       " [('I', 'preferred', 'nsubj'),\n",
       "  ('preferred', 'preferred', 'ROOT'),\n",
       "  ('Los', 'Angeles', 'compound'),\n",
       "  ('Angeles', 'preferred', 'dobj'),\n",
       "  ('.', 'preferred', 'punct')]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(c.text, c.head.text, c.dep_) for c in nlp(sentence.text)]\n",
    "\n",
    " for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.7. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Named Entity Recognition** is the process of classifying words in a sentence based on its noun category (PERSON, FACILITY, ORGANIZATION, GEOPOLITICAL ENTITY, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [[(entity.text, entity.label_) for entity in nlp(sentence.text).ents] for sentence in nlp(documents).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('2', 'CARDINAL'), ('New York', 'GPE'), ('2011', 'DATE')], [], [('Los Angeles', 'GPE')]]\n"
     ]
    }
   ],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù **Your turn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the 7 different methods to preprocess words on the first row of the new's dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìï Hovy, D. (2020). Text Analysis in Python for Social Scientists: Discovery and Exploration. Cambridge University Press.\n",
    "\n",
    "üåç https://medium.com/zero-equals-false/one-hot-encoding-129ccc293cda\n",
    "\n",
    "üåç https://markroxor.github.io/gensim/static/notebooks/word2vec.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
