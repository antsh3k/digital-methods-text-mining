{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Live coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will show you how we analyse a text in real life. To do so, we will examine two judge responses to asylum's claims in the UK.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend of symbols:\n",
    "\n",
    "- ü§ì: Tips\n",
    "\n",
    "- ü§ñüìù: Your turn\n",
    "\n",
    "- ‚ùì: Question\n",
    "\n",
    "- üí´: Extra exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Read text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have learned in this course, the first step is to import the text into this notebook.\n",
    "\n",
    "Two approaches:\n",
    "\n",
    "- 1) Copy and paste content in a **<tt>.txt<tt>** file.\n",
    "- 2) Install **<tt>pdftotext<tt>**: https://github.com/jalan/pdftotext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "# Read the raw file from txt\n",
    "f = open('../data/asylum_claims.txt','r')\n",
    "text = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's substitute \\n by spaces\n",
    "import re\n",
    "text = re.sub('\\n', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(\"../data/PA059452018.pdf\") as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    pdf_11 = first_page.extract_text()\n",
    "    second_page = pdf.pages[1]\n",
    "    pdf_12 = second_page.extract_text()\n",
    "    third_page = pdf.pages[2]\n",
    "    pdf_13 = third_page.extract_text()\n",
    "    fourth_page = pdf.pages[3]\n",
    "    pdf_14 = first_page.extract_text()\n",
    "    fifth_page = pdf.pages[4]\n",
    "    pdf_15 = fifth_page.extract_text()\n",
    "\n",
    "pdf_1 = pdf_11 + \"\\n\" + pdf_12 + \"\\n\" + pdf_13 + \"\\n\" + pdf_14 + \"\\n\" + pdf_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(\"../data/PA002402019.pdf\") as pdf:\n",
    "    first_page = pdf.pages[0]\n",
    "    pdf_11 = first_page.extract_text()\n",
    "    second_page = pdf.pages[1]\n",
    "    pdf_12 = second_page.extract_text()\n",
    "    third_page = pdf.pages[2]\n",
    "    pdf_13 = third_page.extract_text()\n",
    "    fourth_page = pdf.pages[3]\n",
    "    pdf_14 = first_page.extract_text()\n",
    "    fifth_page = pdf.pages[4]\n",
    "    pdf_15 = fifth_page.extract_text()\n",
    "\n",
    "pdf_2 = pdf_11 + \"\\n\" + pdf_12 + \"\\n\" + pdf_13 + \"\\n\" + pdf_14 + \"\\n\" + pdf_15\n",
    "\n",
    "pdf = pdf_1 + \"\\n\" + pdf_2\n",
    "\n",
    "print(pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ì It is important when analysing text to know the basic figures: \n",
    "- How many words do we have? \n",
    "- How many sentences? \n",
    "- What are the most common words? \n",
    "\n",
    "\n",
    "‚ùì More questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1. How many words do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words do we have?\n",
    "words_txt = text.split()\n",
    "len(words_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_pdf = pdf.split()\n",
    "len(words_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in the pdf but not in the txt\n",
    "len(set(words_pdf) ^ set(words_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2. How many sentences do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sentences do we have?\n",
    "sent_txt = text.split('.')\n",
    "len(sent_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sentences do we have?\n",
    "sent_pdf = pdf.split('.')\n",
    "len(sent_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above approach is limited as not all sentences are seperated \n",
    "# with a full stop \n",
    "\n",
    "# How many sentences do we have? Using all types of Punctuation marks\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_txt_nltk = sent_tokenize(text)\n",
    "print(len(sent_txt_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many sentences do we have?\n",
    "sent_pdf_nltk = sent_tokenize(pdf)\n",
    "print(len(sent_pdf_nltk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3. What are the most common words? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most common words? \n",
    "wordfreq_txt = []\n",
    "\n",
    "# count words in text\n",
    "for w in words_txt:\n",
    "    wordfreq_txt.append(words_txt.count(w))\n",
    "    \n",
    "# create a list with words and its frequency\n",
    "word_list = list(set(zip(words_txt, wordfreq_txt)))\n",
    "\n",
    "    \n",
    "print(\"Pairs\\n\" + str(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_value = [0]\n",
    "word = [\"\"]\n",
    "for w in word_list:\n",
    "    compare_value = w[1]\n",
    "    if compare_value > highest_value[0]:\n",
    "        highest_value[0] = compare_value\n",
    "        word[0] = w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word, highest_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most common words? \n",
    "wordfreq_pdf = []\n",
    "\n",
    "# count words in text\n",
    "for w in words_pdf:\n",
    "    wordfreq_pdf.append(words_pdf.count(w))\n",
    "    \n",
    "# create a list with words and its frequency\n",
    "word_list = list(set(zip(words_pdf, wordfreq_pdf)))\n",
    "\n",
    "# function to sort the list by second item of tuple\n",
    "def sort_pairs(tup): \n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order) \n",
    "    # key is set to sort using second element of \n",
    "    # sublist lambda has been used \n",
    "    return(sorted(tup, key = lambda x: x[1], reverse = True))  \n",
    "\n",
    "word_list_sort = sort_pairs(word_list)\n",
    "    \n",
    "print(\"Pairs\\n\" + str(word_list_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(word_list_sort)\n",
    "df.columns = ['words', 'counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df.loc[0:10,:], x='counts', y='words', orientation='h', text = 'words',\n",
    "             labels={\n",
    "                     \"counts\": \"Frequency\",\n",
    "                     \"words\": \"Words\"\n",
    "                 },)\n",
    "fig.layout.yaxis.type = 'category'\n",
    "fig.update_layout(yaxis_categoryorder = 'total ascending')\n",
    "fig.update_layout(yaxis=dict(showticklabels=False))\n",
    "fig.update_traces(texttemplate='%{text}', textposition='auto', marker_color='green')\n",
    "fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',   title={\n",
    "        'text': \"Words Frequency in Tribunal Appeals\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1050,\n",
    "    height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Does that give information of the content?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean the text with some  techniques we have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove capital letters\n",
    "text_clean = ' '.join(w.lower() for w in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean = ' '.join(w for w in text_clean.split() if w not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove puncutaction symbols\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "text_clean = [[token.lemma_ for token in sentence] for sentence in nlp(text_clean).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clean_flat = [word for sent in text_clean for word in sent]\n",
    "text_clean_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.4. Count words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "text_clean_counter = dict(Counter(text_clean_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame.from_dict(text_clean_counter, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.reset_index(level=0, inplace=True)\n",
    "df_clean.columns = ['words', 'counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df_clean.loc[0:10,:], x='counts', y='words', orientation='h', text = 'words',\n",
    "             labels={\n",
    "                     \"counts\": \"Frequency\",\n",
    "                     \"words\": \"Words\"\n",
    "                 },)\n",
    "fig.layout.yaxis.type = 'category'\n",
    "fig.update_layout(yaxis_categoryorder = 'total ascending')\n",
    "fig.update_layout(yaxis=dict(showticklabels=False))\n",
    "fig.update_traces(texttemplate='%{text}', textposition='auto', marker_color='purple')\n",
    "fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',   title={\n",
    "        'text': \"Words Frequency in Tribunal Appeals\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1050,\n",
    "    height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's show the word cloud of this text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "word_cloud = WordCloud(background_color=\"white\", repeat=True)\n",
    "word_cloud.generate(text)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5. What we have learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñüìù Now it's your turn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñüìù Find the word 'EURODAC' using the function **<tt>search<tt>** from the **<tt>re<tt>** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in re.finditer(\"EURODAC\", pdf):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[df_clean['words'].str.contains('eurodac', flags=re.IGNORECASE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñüìù Create a word cloud with different colour pattern using the text from the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = WordCloud(background_color=\"skyblue\", colormap=\"Blues\",repeat=True) #skyblue\n",
    "word_cloud.generate(text)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§ñüìù Remove symbols from **<tt>df_clean<tt>** and plot again the frequency of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_list = []\n",
    "for punc in string.punctuation:\n",
    "    punc_list.extend(punc)\n",
    "\n",
    "punc_list.append(\"‚Äôs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df_clean[~df_clean['words'].isin(punc_list)].sort_values(by=['counts'], ascending=False).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(clean_df.loc[0:10,:], x='counts', y='words', orientation='h', text = 'words',\n",
    "             labels={\n",
    "                     \"counts\": \"Frequency\",\n",
    "                     \"words\": \"Words\"\n",
    "                 },)\n",
    "fig.layout.yaxis.type = 'category'\n",
    "fig.update_layout(yaxis_categoryorder = 'total ascending')\n",
    "fig.update_layout(yaxis=dict(showticklabels=False))\n",
    "fig.update_traces(texttemplate='%{text}', textposition='auto', marker_color='purple')\n",
    "fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide',   title={\n",
    "        'text': \"Words Frequency in Tribunal Appeals\",\n",
    "        'y':0.95,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1050,\n",
    "    height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
